{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25lro6L2bVx"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranksense/Twittorials/blob/master/robots_txt_Sitemap_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hl2WtsFaFMh"
      },
      "source": [
        "# üêçüî•\n",
        "#  Is your `robots.txt` file blocking any of the URLs in your sitemap?  \n",
        "\n",
        "We know this generally shouldn't happen, but we also know that it might!  \n",
        "\n",
        "Is it correctly blocking certain URLs for certain User-agents, or is there an error somewhere?  \n",
        "This is quick testing tool that allows you to do just that. \n",
        "\n",
        "The plan: \n",
        "\n",
        "1. Specify a `robots.txt` file URL\n",
        "2. Extract and select one of the available sitemaps\n",
        "3. Download the sitemap\n",
        "4. Extract the `User-agent`s\n",
        "5. For each (User-agent, URL) combination, check whether the user-agent can fetch the URL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FXZPTz7idfR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install advertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuC7wVawijqQ"
      },
      "source": [
        "Import the required packages, and display their version numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2WE1c3XVwdH"
      },
      "outputs": [],
      "source": [
        "import advertools as adv\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = None\n",
        "for p in [adv, pd]:\n",
        "    print(f'{p.__name__:<13}', 'v' + p.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RvSV2dtdWM9d"
      },
      "outputs": [],
      "source": [
        "#@title Please enter a robots.txt URL e.g. https://www.nytimes.com/robots.txt\n",
        "robotstxt_url = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_0Hvp2eWUR5"
      },
      "outputs": [],
      "source": [
        "robotstxt_df = adv.robotstxt_to_df(robotstxt_url)\n",
        "robotstxt_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qithSoXczeb"
      },
      "source": [
        "#### Extract sitemaps from `robots.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTtbN6EVWils"
      },
      "outputs": [],
      "source": [
        "sitemaps = (robotstxt_df                             # take the robotstxt_df DataFrame\n",
        "            [robotstxt_df['directive']               # select its \"directive\" column\n",
        "             .str.contains('^sitemap$', case=False)] # filter values that contain \"sitemap\" case insensitive\n",
        "            ['content']                              # now select the \"content\" column\n",
        "            .tolist())                               # convert it to a list\n",
        "\n",
        "sitemaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jJVT7Ir_XQyd"
      },
      "outputs": [],
      "source": [
        "#@title Enter one of the sitemaps extracted (if none exist, try to get one from the website). If you provide a sitemap index, you will get all the sub-sitemaps (might take long with large websites). Try with a regular sitemap for faster results.\n",
        "sitemap = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkf1WvdtcvTa"
      },
      "source": [
        "#### Download selected sitemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKZOhcJDYbuY"
      },
      "outputs": [],
      "source": [
        "sitemap_df = adv.sitemap_to_df(sitemap)\n",
        "print('Sitemap rows:', sitemap_df.shape[0])\n",
        "print('Sitemap columns:', sitemap_df.shape[1])\n",
        "sitemap_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_a6D-hXcoPL"
      },
      "source": [
        "#### Extract `User-agent`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZuEyr8PY0uo"
      },
      "outputs": [],
      "source": [
        "user_agents = (robotstxt_df                              # take the robotstxt_df DataFrame\n",
        "               [robotstxt_df['directive']                # select its \"directive\" column\n",
        "                .str.contains('user-agent',case=False)]  # filter values that contain \"user-agent\" case insensitive\n",
        "               ['content']                               # now select the \"content\" column\n",
        "               .tolist())                                # convert it to a list\n",
        "user_agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4M9O4RDetwx"
      },
      "source": [
        "#### Run the report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvLPonVKYp5t"
      },
      "outputs": [],
      "source": [
        "robots_test_report = adv.robotstxt_test(robotstxt_url, user_agents, sitemap_df['loc'])\n",
        "robots_test_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeYw95N62bV-"
      },
      "source": [
        "### Homework\n",
        "\n",
        "Now that you have parsed the XML sitemap, and got all its URLs, how can you further analyze those URLs?\n",
        "\n",
        "Hint: `adv.url_to_df`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "robots_txt_sitemap_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}